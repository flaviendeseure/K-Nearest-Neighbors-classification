# -*- coding: utf-8 -*-
"""KNN-DIA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XyNjDrzsAAY16nk80WeRYVAhRLDrzW3x
"""

import warnings
warnings.filterwarnings("ignore")

import sys

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler

from sklearn.model_selection import train_test_split

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import learning_curve

from sklearn.exceptions import NotFittedError
from scipy.stats import mode
from numpy.linalg import norm

"""# KNN

## Importation des données
"""

colnames = [f"Var{i}" for i in range(1,7)]
colnames.append("Class")
dataset = pd.read_csv("data.csv",names=colnames)
dataset["Class"] = dataset["Class"].apply(lambda x: x[-1])
dataset.head()

colnames = [f"Var{i}" for i in range(1,7)]
colnames.append("Class")
dataset_pretest = pd.read_csv("data.csv",names=colnames)
dataset_pretest["Class"] = dataset_pretest["Class"].apply(lambda x: x[-1])
dataset_pretest.head()

colnames = [f"Var{i}" for i in range(1,7)]
dataset_final = pd.read_csv("finalTest.csv",names=colnames)
dataset_final.head()

dataset.describe()

dataset_pretest.describe()

dataset_final.describe()

"""## A- Exploratory Data Analysis
### Objectif :
- Comprendre du mieux possible nos données  

"""

df = dataset.copy()

"""### Analyse de Forme :  
- **Variables et target**:   
Il y a 6 variables et une target Class.

- **Lignes**:
"""

df.shape

"""Il y a 803 lignes dans le dataset.

- **Types de variables**:
"""

df[colnames[:-1]].dtypes.value_counts()

"""Les variables sont quantatives continue.

- **Analyse des valeurs manquantes**:
"""

(df.isna().sum()/df.shape[0]).sort_values(ascending=True)

"""Le dataset est complet (pas de valeurs manquantes).

### Analyse de Fond :
- **Visualisation de la target**:
"""

df['Class'].value_counts(normalize=True)

"""Les classes A et B sont les plus fréquentes.  
Laclasse C est très peu présente.

- **Signification des variables**:
"""

plt.figure(figsize=(18,8))
index = 1
for col in df.select_dtypes('float64'):
    plt.subplot(2,3,index)
    sns.distplot(df[col])
    index += 1

plt.savefig("signification_variables")
plt.show()

"""Centrer et réduire les données est une piste à envisager.

- **Relation Variables/Target**:
"""

index=1
row = 1
column = 1
plt.figure(figsize=(18,8))
for col in df.select_dtypes('float64'):
    plt.subplot(2,3,index)
    sns.distplot(df[df["Class"]=="A"][col], label='A')
    sns.distplot(df[df["Class"]=="B"][col], label='B')
    sns.distplot(df[df["Class"]=="C"][col], label='C')
    sns.distplot(df[df["Class"]=="D"][col], label='D')
    sns.distplot(df[df["Class"]=="E"][col], label='E')
    plt.legend()
    index += 1

plt.savefig("relation_variable_target")
plt.show()

"""Groupes:
- **Var 1:** C, D, AB, E
- **Var 2:** CD, AB, E
- **Var 3:** ACE, B, D
- **Var 4:** C, D, ABE
- **Var 5:** Pas utile 
- **Var 6:** D, B, CAE

- **Relation Variables/Variables**
"""

plt.figure()
sns.heatmap(df.select_dtypes("float64").corr())
plt.savefig("relation_entre_variables")
plt.show()

"""Les variables 3 et 6 sont très corrélées.  
Une seule des variables 3 ou 6 suffit.
"""

plt.figure()
sns.pairplot(df.select_dtypes("float64"))
plt.savefig("relation_entre_variables_pairplot")
plt.show()

"""Avec ses graphiques on peut vérifier les affirmations précédentes:  
1) la variable 5 n'est pas du tout intéressante pour la régression  
2) il y a une corrélation forte entre les variables 3 et 6
"""

dataset.columns

for i in dataset.drop("Class",axis=1).columns:
  for j in dataset.drop("Class",axis=1).columns:
    if i!=j:
      sns.lmplot(i,j, data=dataset, hue='Class')

"""### Conclusion
Les variables 1, 2, 3 et 4 semblent les plus intéressante pour notre problème (à vérifier dans la suite).   
On prendra éventuellement la variable 6 mais elle a un faible impact.

## B- Preprocessing
Le preprocessing consiste en plusieurs étapes:   
1) Encodage des données (pour les données qualitative)  
2) Normalisation des données  
3) Imputation (remplacer les données manquantes)  
4) Sélection de variables  
5) Extraction  
  
Les étapes 1 et 3 n'ont pas besoin d'être réalisées ici
"""

trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)

trainset["Class"].value_counts(normalize = True)

testset["Class"].value_counts(normalize = True)

"""**Normalisation**   
L'objectif est de recentrer les données et de les mettre sur une même échelle.  
Il existe plusieurs stratégies pour cela:  
- MinMax : $\frac{X-X_{min}}{X_{max}-X_{min}}$ toutes les valeurs sont comprises entre 0 et 1
- Standardisation : $X_{scaled}=\frac{X-\mu_X}{\sigma_X}$ on centre et réduit les données
- RobustScaler: $X_{scaled}=\frac{X-mediane}{IQR}$ permet d'avoir une échelle peu sensible aux outliers grâce à l'utilisation de la médiane et de l'écart interquartile
"""

min_max = MinMaxScaler()
standard = StandardScaler()
robust = RobustScaler()

"""## C- Modelisation

### Algorithme
"""

# Définition des distances testées :
def euclidian_distance_(p1,p2):
  return sum([(i-j)**2  for i, j in zip(p1,p2)])**0.5

def manhattan_distance(p1,p2):
  return sum([abs(i-j) for i, j in zip(p1,p2)])

def minkowski_distance(p1,p2,p=3):
  return sum([abs(i-j)**p  for i, j in zip(p1,p2)])**1/p

def cosine_distance(p1,p2):
  return np.vdot(p1,p2)/(norm(p1)*norm(p2))

def convert2numpy(data):
  if isinstance(data, pd.core.frame.DataFrame):
    return data.to_numpy()
  elif isinstance(data, list): 
    return np.array(data)
  elif isinstance(data,np.ndarray):
    return data
  else:
    raise TypeError("Les données doivent être des types suivants:\npandas.core.frame.DataFrame\nlist\nnumpy.ndarray")

class DataError(Exception):
  """
  Les labels sont présents dans les données
  """
  def __init__(self, name):
    self.name = name

  def __str__ (self):
      return f"Les labels sont présents dans les données."

class knn_:
  def __init__(self, k, metric = euclidian_distance_):
    self.data = None
    self.k = k if k > 0 else 1  
    self.metric = metric

  def fit(self, train):
    self.data = convert2numpy(train)
    if isinstance(train, pd.core.frame.DataFrame):
      self.target_names=sorted(train.iloc[:,-1].unique())
    elif isinstance(train, (np.ndarray,list)):
      self.target_names=sorted(pd.DataFrame(train).iloc[:,-1].unique())
  
  def set_target_names(self, liste):
    if isinstance(liste, (list,np.ndarray)):
      self.target_names_ = liste
  def get_target_names(self):
    return self.target_names_

  target_names = property(get_target_names, set_target_names)

  def score(self, test):
    if self.data is not None:
      test = convert2numpy(test)
      bon_val = 0
      for x in test:
        k_neighbors = sorted(self.data,key = lambda xi: self.metric(xi[:-1],x[:-1]))[:self.k]
        bon_val += 1 if mode(k_neighbors).mode[0][-1] == x[-1] else 0
      return bon_val/len(test)
    else:
      raise NotFittedError(f"This knn_ instance is not fitted yet. Call 'fit' with appropriate arguments before using the '{self.score.__name__}' function.")

  def crossValidation(self, fold_nb = 5):
    data = self.cv(fold_nb)
    result = list()
    for i in range(fold_nb):
      test = data[i]
      train = np.delete(data,i,axis=0)
      train = np.reshape(train, (len(train[0])*len(train),len(train[0][0])))
      bon_val = 0
      for x in test:
        k_neighbors = sorted(train,key = lambda xi: self.metric(list(map(float,xi[:-1])),list(map(float,xi[:-1]))))[:self.k]
        bon_val += 1 if mode(k_neighbors).mode[0][-1] == x[-1] else 0
      result.append(bon_val/len(test))
    return np.mean(result), result, self.score(data[0])

  def cv(self, fold_nb = 5):
    if self.data is not None:
      fold_size = len(self.data)//fold_nb
      folds = list()
      train = self.data.copy().tolist()
      for k in range(fold_nb):
        fold = list()
        while len(fold) < fold_size:
          val = np.random.randint(0, len(train)-1)
          fold.append(train.pop(val)) 
        folds.append(fold)
      return np.array(folds)
    else:
      raise NotFittedError(f"This knn_ instance is not fitted yet. Call 'fit' with appropriate arguments before using the '{self.cv.__name__}' function.")
  
  def find_best_k(self, validation, k_min = 1, k_max = 15):
    if self.data is not None:
      if isinstance(k_min, int) and isinstance(k_max, int):
        if k_min > 0 and k_max > k_min:
          best_params_values = dict()
          for k in range(k_min, k_max):
            self.k = k
            best_params_values[k] = self.score(validation)
          best = max(best_params_values.items(),key=lambda x: x[1])
          self.k = best[0]
          return best
        else:
          raise ValueError("k_min ou k_max n'est pas positif")
      else:
        raise TypeError("k_min ou k_max n'est pas un entier")
    else:
      raise NotFittedError(f"This knn_ instance is not fitted yet. Call 'fit' with appropriate arguments before using the '{self.score.__name__}' function.")

  def predict(self, X, save = True):
    if self.data is not None:
      X = convert2numpy(X)
      if type(X[0][-1]) is not str:
        if not isinstance(X[0],np.ndarray):
          k_neighbors = sorted(self.data,key = lambda xi: self.metric(x,X))[:self.k]
          return mode(k_neighbors).mode[0][-1]
        else:
          val = list()
          for x in X:
            k_neighbors = sorted(self.data,key = lambda xi: self.metric(xi,x))[:self.k]
            val.append(mode(k_neighbors).mode[0][-1])

          if save is not None:
            with open(f"deseure--charron_sample.txt", "w") as f:
              f.writelines(list(map(lambda element: f"class{element}\n",val[:-1])))
              f.write(f"class{val[-1]}")
          return val
      else:
        raise DataError("Error")
    else:
      raise NotFittedError(f"This knn_ instance is not fitted yet. Call 'fit' with appropriate arguments before using the '{self.predict.__name__}' function.")

  def convert_labels_to_int(self,y):
    return self.target_names.index(y)

  def confusion_matrix_(self,y_true, y_pred):
    mat = np.zeros((len(self.target_names),len(self.target_names)))
    for i,j in zip(y_true, y_pred):
      mat[self.convert_labels_to_int(i[-1])][self.convert_labels_to_int(j)] += 1
    return mat

model = knn_(3)

"""### 1. Courbe d'apprentissage"""

X = df.iloc[:,:-1]
y = df.iloc[:,-1]
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 0)

model_knn = KNeighborsClassifier(5)
N, train_score, val_score = learning_curve(model_knn, X_train, y_train,cv=5)

plt.figure()
plt.plot(N, train_score.mean(axis=1), label ="train score")
plt.plot(N, val_score.mean(axis=1), label = "val_score")
plt.title("Courbe d'apprentissage")
plt.savefig("courbe_apprentissage")

"""Il manque des données.

### 2. Suppression de la cinquième variable

#### a) Avant suppression
"""

trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)
X_test,y_test = testset.iloc[:,:-1],testset.iloc[:,-1]

model.fit(trainset)
print(model.find_best_k(testset))

"""####b) Après suppression"""

df = dataset.drop("Var5",axis=1)
trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)
X_test,y_test = testset.iloc[:,:-1],testset.iloc[:,-1]

model.fit(trainset)
print(model.find_best_k(testset))

"""Conclusion: La variable 5 n'apporte aucun avantage, on peut l'enlever pour le moment. On verra par la suite si le choix était justifié.

### 3. Modification de la distance
"""

trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)
X_test,y_test = testset.iloc[:,:-1],testset.iloc[:,-1]

val = dict()
for function_distance in [euclidian_distance_, manhattan_distance ,minkowski_distance, cosine_distance]:
  model = knn_(3, function_distance)
  model.fit(trainset)
  val[function_distance.__name__] = model.find_best_k(testset)
print(val)

"""**Meilleurs distances:** euclidienne et minkowski_distance  
On utilisera la distance euclidienne par la suite

### 4. Normalisation des données

#### a) Sans normalisation
"""

df = dataset.drop("Var5",axis=1)
trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)
model.fit(trainset)
print(model.find_best_k(testset))

"""#### b) Avec normalisation"""

val = dict()
for function_normalize, name in zip([min_max, standard, robust], ["MinMax","Standard","Robust"]):
  df = pd.DataFrame(function_normalize.fit_transform(dataset.iloc[:,:-1].drop("Var5", axis=1)))
  df.columns = colnames[:4] + colnames[5:]
  df["Class"] = dataset.iloc[:,-1]
  trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)
  model = knn_(3)
  model.fit(trainset)
  val[name] = model.find_best_k(testset)
print(val)

"""La meilleur méthode est la standardisation

### Conclusion
Testons maintenant avec et sans la variable 5:

#### a) Avec la Var5
"""

df = pd.DataFrame(standard.fit_transform(dataset.iloc[:,:-1]))
df.columns = colnames
df["Class"] = dataset.iloc[:,-1]
trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)
model = knn_(3)
model.fit(trainset)
print(model.find_best_k(testset))

"""#### b) Sans la Var5"""

df = pd.DataFrame(standard.fit_transform(dataset.iloc[:,:-1].drop("Var5", axis=1)))
df.columns = colnames[:4] + colnames[5:]
df["Class"] = dataset.iloc[:,-1]
trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)
model = knn_(3)
model.fit(trainset)
print(model.find_best_k(testset))

"""#### Analyse de la matrice de confusion:"""

df = pd.DataFrame(standard.fit_transform(dataset.iloc[:,:-1].drop("Var5", axis=1)))
df.columns = colnames[:4] + colnames[5:]
df["Class"] = dataset.iloc[:,-1]
trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)
model = knn_(5)
model.fit(trainset)
y_pred = model.predict(testset.iloc[:,:-1])
model.confusion_matrix_(testset.iloc[:,-1],y_pred)

"""On remarque que le modèle confond les labels A et B

#### La meilleur combinaison trouvée est:  
*Var5:* sans  
*Distance:* euclidienne  
*Normalisation:* standardisation    
*k* = 5 (3 si données pas normalisé)

### 5. KNN pondéré

#### Algo
"""

class Weighted_KNN:
  def __init__(self, k, metric = euclidian_distance_):
    self.data = None
    self.k = k if k > 0 else 1  
    self.metric = metric

  def fit(self, train):
    self.data = convert2numpy(train)
    if isinstance(train, pd.core.frame.DataFrame):
      self.target_names=sorted(train.iloc[:,-1].unique())
    elif isinstance(train, (np.ndarray,list)):
      self.target_names=sorted(pd.DataFrame(train).iloc[:,-1].unique())
  
  def set_target_names(self, liste):
    if isinstance(liste, (list,np.ndarray)):
      self.target_names_ = liste
  def get_target_names(self):
    return self.target_names_

  target_names = property(get_target_names, set_target_names)

  def score(self, test):
    if self.data is not None:
      test = convert2numpy(test)
      bon_val = 0
      for x in test:
        distance = sorted([(self.metric(xi[:-1],x[:-1]),xi[-1]) for xi in self.data])[:self.k]
        freq = {classe:0 for classe in self.target_names}
        for d in distance:
          freq[d[1]] += 1/d[0] 
        bon_val += 1 if sorted(freq.items(), key=lambda x: x[1], reverse=True)[0][0] == x[-1] else 0
      return bon_val/len(test)
    else:
      raise NotFittedError(f"This knn_ instance is not fitted yet. Call 'fit' with appropriate arguments before using the '{self.cv.__name__}' function.")

  def predict(self, X):
    if self.data is not None:
      X = convert2numpy(X)
      result = list()
      for x in X:
        distance = sorted([(self.metric(xi[:-1],x),xi[-1]) for xi in self.data])[:self.k]
        freq = {classe:0 for classe in self.target_names}
        for d in distance:
          freq[d[1]] += 1/d[0] 
        result.append(sorted(freq.items(), key=lambda x: x[1], reverse=True)[0][0])
      return result
    else:
      raise NotFittedError(f"This knn_ instance is not fitted yet. Call 'fit' with appropriate arguments before using the '{self.cv.__name__}' function.")

  def convert_labels_to_int(self,y):
    return self.target_names.index(y)

  def confusion_matrix_(self,y_true, y_pred):
    mat = np.zeros((len(self.target_names),len(self.target_names)))
    for i,j in zip(y_true, y_pred):
      mat[self.convert_labels_to_int(i[-1])][self.convert_labels_to_int(j)] += 1
    return mat

"""#### a) En fonction de la distance"""

df = pd.DataFrame(standard.fit_transform(dataset.iloc[:,:-1].drop("Var5", axis=1)))
df.columns = colnames[:4] + colnames[5:]
df["Class"] = dataset.iloc[:,-1]
trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)

res = list()
for k in range(1,70):
  model = Weighted_KNN(k)
  model.fit(trainset)
  res.append(model.score(testset))
print(max(res),np.argmax(res)+1)

"""Le KNN pondéré en fonction de la distance ne donne pas de meilleurs résultats

### Score sur le dataset preTest

#### a) KNN normal
"""

df_pretest = pd.DataFrame(standard.fit_transform(dataset_pretest.iloc[:,:-1].drop("Var5", axis=1)))
df_pretest.columns = colnames[:4] + colnames[5:]
df_pretest["Class"] = dataset_pretest.iloc[:,-1]
model = knn_(5)
model.fit(trainset)
model.score(df_pretest)

"""#### b) KNN pondéré selon la distance"""

df_pretest = pd.DataFrame(standard.fit_transform(dataset_pretest.iloc[:,:-1].drop("Var5", axis=1)))
df_pretest.columns = colnames[:4] + colnames[5:]
df_pretest["Class"] = dataset_pretest.iloc[:,-1]
model = Weighted_KNN(6)
model.fit(trainset)
print(model.score(testset))

"""## D- Rendu"""

def isCorrect(result,testset):
  allLabels = ['classA','classB','classC','classD','classE']
  nbLines = testset.shape[0]

  with open('deseure--charron_sample.txt','r') as fd:
    lines = fd.readlines()

  count=0
  for label in lines:
    if label.strip() in allLabels:
      count+=1
    else:
      if count<nbLines:
        print("Wrong label line:"+str(count+1))
        break
  if count<nbLines:
    return False
  else:
    return True

df = pd.DataFrame(standard.fit_transform(dataset.iloc[:,:-1].drop("Var5", axis=1)))
df.columns = colnames[:4] + colnames[5:]
df["Class"] = dataset.iloc[:,-1]
trainset, testset = train_test_split(df, test_size = 0.25, random_state = 0)

df_final = dataset_final.drop("Var5", axis=1)
df_final = pd.DataFrame(standard.fit_transform(df_final))

model = knn_(5)
model.fit(trainset)
result = model.predict(df_final,True)
if isCorrect(result,df_final):
  print("Labels Check : Successfull!")
  result_dico = {classe:nb for classe, nb in zip(np.unique(result, return_counts=True)[0],np.unique(result, return_counts=True)[1])}
  result_dico = sorted(result_dico.items(), key=lambda x: x[1], reverse = True)
  for classe, nb in result_dico:
    print(classe, nb, sep = "    ") 
  print("\nProportion df_final")
  for classe, nb in result_dico:
    print(classe, round(nb/len(df_final),6), sep = "    ") 
  print("\nProportion data")
  print(dataset["Class"].value_counts(normalize = True))
  print("\nProportion pre_test")
  print(dataset_pretest["Class"].value_counts(normalize = True))
  print("\nOn remarque que la proportion est toujours la même pour les deux datasets.")
  print("Sur ce constat, on peut calculer la différence de proportion: ")
  result_nb = [nb[1]/len(df_final) for nb in result_dico]
  erreurs = list()
  for originale, predite in zip(dataset_pretest["Class"].value_counts(normalize = True),result_nb):
    erreurs.append(abs(predite-originale)) 
  print("%.2f"%(sum(erreurs)*100),"% d'erreur.")
else:
  print("Labels Check : fail!")